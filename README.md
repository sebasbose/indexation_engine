# Motor de BÃºsqueda Distribuido

Sistema de bÃºsqueda distribuido desarrollado para el curso de Bases de Datos Avanzadas. Este proyecto implementa un motor de bÃºsqueda completo con crawler, procesamiento de mensajes con Kafka, almacenamiento distribuido en mÃºltiples bases de datos y una interfaz web.

## ğŸ“š DocumentaciÃ³n

### ğŸš€ Empieza AquÃ­
1. **[Resumen Ejecutivo](docs/RESUMEN_EJECUTIVO.md)** â­ - VisiÃ³n general completa
2. **[Inicio RÃ¡pido](docs/QUICKSTART.md)** â­ - CÃ³mo empezar en 3 pasos
3. **[Checklist](docs/CHECKLIST.md)** â­ - VerificaciÃ³n paso a paso

### ğŸ“– DocumentaciÃ³n Completa
- **[Ãndice](docs/INDICE.md)** - NavegaciÃ³n de toda la documentaciÃ³n
- **[Plan de Trabajo](docs/PLAN_DE_TRABAJO.md)** - Roadmap de 3 dÃ­as
- **[Arquitectura](docs/arquitectura.md)** - DiseÃ±o tÃ©cnico detallado
- **[Diagramas](docs/DIAGRAMAS.md)** - Diagramas visuales del sistema
- **[Debugging](docs/DEBUGGING.md)** - Comandos Ãºtiles para troubleshooting
- **[Notas de ImplementaciÃ³n](docs/NOTAS_IMPLEMENTACION.md)** - Decisiones de diseÃ±o

## Arquitectura

El sistema estÃ¡ compuesto por los siguientes componentes:

### Servicios

1. **Crawler (Scrapy + Python)**
   - Crawlea pÃ¡ginas web y extrae metadatos
   - EnvÃ­a datos a Kafka para procesamiento asÃ­ncrono
   - UbicaciÃ³n: `services/crawler/`

2. **Ingest (Python)**
   - Consume mensajes de Kafka
   - Almacena datos en 3 bases de datos:
     - **MySQL**: Metadatos (tÃ­tulo, URL, descripciÃ³n)
     - **MongoDB**: Contenido completo de las pÃ¡ginas
     - **PostgreSQL**: Ãndice invertido para bÃºsquedas
   - UbicaciÃ³n: `services/ingest/`

3. **API (Node.js + Express)**
   - Gateway que agrega consultas de las 3 bases de datos
   - Endpoint de bÃºsqueda distribuida
   - UbicaciÃ³n: `services/api/`

4. **Web (Next.js + React)**
   - Interfaz de usuario para bÃºsquedas
   - Muestra resultados y estadÃ­sticas
   - UbicaciÃ³n: `services/web/`

### Infraestructura

- **Kafka + Zookeeper**: Cola de mensajes para procesamiento asÃ­ncrono
- **MySQL**: Almacenamiento de metadatos
- **PostgreSQL**: Ãndice invertido
- **MongoDB**: Almacenamiento de contenido

## Requisitos Previos

- Docker y Docker Compose instalados
- Al menos 4GB de RAM disponible

## InstalaciÃ³n y EjecuciÃ³n

### 1. Levantar la infraestructura

```bash
# Levantar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f

# Ver logs de un servicio especÃ­fico
docker-compose logs -f crawler
docker-compose logs -f ingest
docker-compose logs -f api
```

### 2. Inicializar las bases de datos

Los esquemas se inicializan automÃ¡ticamente cuando el servicio de ingest se conecta por primera vez.

### 3. Acceder a los servicios

- **Web UI**: http://localhost:3000
- **API**: http://localhost:3001
- **MySQL**: localhost:3306
- **PostgreSQL**: localhost:5432
- **MongoDB**: localhost:27017
- **Kafka**: localhost:9092

## Uso

### Realizar bÃºsquedas

1. Abre http://localhost:3000 en tu navegador
2. Ingresa tu consulta en la barra de bÃºsqueda
3. Los resultados se mostrarÃ¡n con tÃ­tulo, URL, snippet y score

### Endpoints de la API

#### BÃºsqueda
```bash
GET /api/search?q=database&page=1&limit=10
```

#### EstadÃ­sticas
```bash
GET /api/stats
```

#### Health Check
```bash
GET /api/health
```

## Desarrollo

### Estructura del Proyecto

```
indexation_engine/
â”œâ”€â”€ docker-compose.yml          # ConfiguraciÃ³n de Docker
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ crawler/               # Spider de Scrapy
â”‚   â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”‚   â”œâ”€â”€ spiders/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ web_spider.py
â”‚   â”‚   â”‚   â””â”€â”€ pipelines.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ ingest/                # Consumer de Kafka
â”‚   â”‚   â”œâ”€â”€ consumer.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”œâ”€â”€ api/                   # API Gateway
â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â””â”€â”€ web/                   # Frontend Next.js
â”‚       â”œâ”€â”€ pages/
â”‚       â”œâ”€â”€ styles/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ scripts/               # Scripts de inicializaciÃ³n
â”‚       â”œâ”€â”€ init-mysql.sql
â”‚       â””â”€â”€ init-postgres.sql
â””â”€â”€ docs/
    â””â”€â”€ planteamiento.md       # EspecificaciÃ³n del proyecto
```

### Agregar nuevas URLs para crawlear

Edita el archivo `services/crawler/crawler/spiders/web_spider.py` y modifica la lista `start_urls`:

```python
start_urls = [
    'https://example.com',
    'https://another-site.com',
]
```

### Modificar lÃ­mite de pÃ¡ginas

En el mismo archivo, cambia el valor de `CLOSESPIDER_PAGECOUNT`:

```python
custom_settings = {
    'CLOSESPIDER_PAGECOUNT': 500,  # Cambiar este nÃºmero
}
```

## Detener el Sistema

```bash
# Detener todos los servicios
docker-compose down

# Detener y eliminar volÃºmenes (borra datos)
docker-compose down -v
```

## Troubleshooting

### Los contenedores no inician

```bash
# Ver logs de errores
docker-compose logs

# Reiniciar un servicio especÃ­fico
docker-compose restart [service-name]
```

### No se estÃ¡n indexando pÃ¡ginas

1. Verifica que Kafka estÃ© funcionando: `docker-compose logs kafka`
2. Verifica que el crawler estÃ© enviando mensajes: `docker-compose logs crawler`
3. Verifica que el ingest estÃ© procesando: `docker-compose logs ingest`

### No aparecen resultados en bÃºsquedas

1. Verifica que hay documentos indexados: http://localhost:3001/api/stats
2. Verifica la conectividad de las bases de datos: http://localhost:3001/api/health
3. Revisa los logs de la API: `docker-compose logs api`

## CaracterÃ­sticas Implementadas

- âœ… Crawler distribuido con Scrapy
- âœ… Cola de mensajes con Kafka
- âœ… Almacenamiento distribuido (MySQL, PostgreSQL, MongoDB)
- âœ… Ãndice invertido para bÃºsquedas eficientes
- âœ… API Gateway que agrega resultados
- âœ… Interfaz web con Next.js
- âœ… TokenizaciÃ³n y scoring bÃ¡sico
- âœ… ContenerizaciÃ³n completa con Docker

## Mejoras Futuras

- [ ] Implementar rÃ©plicas de bases de datos
- [ ] Agregar particionamiento de datos
- [ ] Mejorar algoritmo de ranking (TF-IDF)
- [ ] Implementar paginaciÃ³n completa
- [ ] Agregar autenticaciÃ³n
- [ ] Implementar cache de resultados
- [ ] Agregar mÃ©tricas y monitoreo

## Autores

Proyecto desarrollado para el curso de Bases de Datos Avanzadas - PUCP

## Licencia

MIT
